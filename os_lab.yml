## OpenStack on OpenStack lab setup
## Author: Cees Moerkerken
## 
## Will create a virtual openstack setup on top of openstack
## Only intended for testing purposes!!!
##
## Run as
## TF_STATE=./tf_openstack/terraform.tfstate ansible-playbook os_lab.yml"
## When using a vault add: "--ask-vault-pass"
##
## ASCII genereted with  pyfiglet -f basic "name"

---

## install python via raw module
- hosts: type_openstack_compute_instance_v2
  gather_facts: False

  tasks:
  - name: install python 2
    raw: |
      test -e /usr/bin/python || \
       (sudo apt -y update && sudo apt install -y python-minimal)
    tags: firstrun

#######################################################################
#  .o88b.  .d88b.  .88b  d88. .88b  d88.  .d88b.  d8b   db 
# d8P  Y8 .8P  Y8. 88'YbdP`88 88'YbdP`88 .8P  Y8. 888o  88 
# 8P      88    88 88  88  88 88  88  88 88    88 88V8o 88 
# 8b      88    88 88  88  88 88  88  88 88    88 88 V8o88 
# Y8b  d8 `8b  d8' 88  88  88 88  88  88 `8b  d8' 88  V888 
#  `Y88P'  `Y88P'  YP  YP  YP YP  YP  YP  `Y88P'  VP   V8P 
#
## common stuff
#######################################################################
- hosts: all
  gather_facts: True
  tasks:
  - name: "Ansible setup"
    setup:
    tags: always

- hosts: type_openstack_compute_instance_v2
  tasks:
  - name: "Build hosts file"
    lineinfile: 
      dest=/etc/hosts 
      regexp='{{ hostvars[item].access_ip_v4 }}.*' 
      line="{{ hostvars[item].access_ip_v4 }} {{ hostvars[item].ansible_hostname }}" 
      state=present
    with_items: 
      - "{{ groups['type_openstack_compute_instance_v2'] }}"
    tags: 
      - firstrun
      - hostsfile

  - name: "fix slow ssh on ubuntu"
    lineinfile: 
      dest=/etc/pam.d/common-session
      regexp='.*session.*optional.*pam_systemd.so' 
      line='#session optional        pam_systemd.so'
      insertbefore='# end of pam-auth-update config'
      firstmatch=yes
      state=present
    tags: 
      - firstrun
      - pam-fix
    notify: restart-sshd

  - name: apt repositories and packages
    block:
      - name: add apt keys
        apt_key:
          id: "{{openstack_apt_key}}"
          keyserver: keyserver.ubuntu.com
          state: present

      - name: add-apt-repository for OpenStack
        apt_repository:
          repo: "{{openstack_repo}}"
        tags: add-apt-repository

      - name: add ceph apt keys
        apt_key:
          url: https://download.ceph.com/keys/release.asc
          state: present
        tags: 
          - add-apt-repository
          - ceph

      - name: add-apt-repository ceph
        apt_repository:
          repo: "{{ceph_repo}}"
          filename: ceph.list
        tags: 
          - add-apt-repository
          - ceph

      # - name: enable auto-updates and reboot since this is for testing
      #   copy:
      #     src: 50unattended-upgrades
      #     dest: /etc/apt/apt.conf.d/50unattended-upgrades
      #   tags: auto-updates

      - name: dist-upgrade
        apt:
          upgrade: dist
          update_cache: yes
          dpkg_options: 'force-confold,force-confdef'
          autoremove: yes
        tags: firstrun
        ##retry so we can use forking
        register: apt_result
        retries: 10
        until: apt_result is success

      - name: ensure common packages are installed
        apt:
          pkg:
            - chrony
            - software-properties-common
            - python-openstackclient 
          state: present
          update_cache: yes
          autoremove: yes
        register: apt_result
        retries: 10
        until: apt_result is success

      - name: ensure group_vars specific packages are installed
        apt:  pkg={{packages}} state=present update_cache=yes
        register: apt_result
        retries: 10
        until: apt_result is success
    tags: packages

  - name: configure chrony
    lineinfile:
      path=/etc/chrony/chrony.conf
      regexp='^allow'
      line='allow 10.0.0.0/24'
    notify: restart-chrony

  - name: configure vim (syntax highligt .conf files as ini)
    lineinfile:
      path=/etc/vim/vimrc
      line='autocmd BufRead,BufNewFile *.conf setf dosini'

  - name: set timezone
    timezone:
      name: Europe/Bratislava

  - name: template rc files on all nodes (might come in handy)
    template:
      src=files/{{item}}.j2
      dest=/root/{{item}}
      backup=yes
    with_items:
      - adminrc
      - userrc
      - demorc
    tags: rc

  - name: template rc files on all nodes (might come in handy)
    template:
      src=files/{{item}}.j2
      dest=/home/ubuntu/{{item}}
      backup=yes
    with_items:
      - adminrc
      - userrc
      - demorc
    tags: rc

  - name: add adminrc to bashrc on all nodes
    lineinfile:
      path={{item}}
      line='. ~/adminrc'
    with_items:
      - /home/ubuntu/.bashrc
    tags: rc

  - name: configure sysctl
    sysctl:
      name: "{{item.key}}"
      value: "{{item.value}}"
    with_dict:
      - net.bridge.bridge-nf-call-iptables: 1
      - net.bridge.bridge-nf-call-ip6tables: 1
      - vm.overcommit_memory: 1
      - net.core.somaxconn: 1
      - net.ipv6.conf.all.disable_ipv6: 1
      - net.ipv6.conf.default.disable_ipv6: 1
      - net.ipv6.conf.lo.disable_ipv6: 1
    register: sysctl_result
    tags: sysctl
    ##fails and retry doesnt work, so ignore errors
    ignore_errors: yes

    ##and try again the ugly way
  - name: configure sysctl
    sysctl:
      name: "{{item.key}}"
      value: "{{item.value}}"
    with_dict:
      - net.bridge.bridge-nf-call-iptables: 1
      - net.bridge.bridge-nf-call-ip6tables: 1
      - vm.overcommit_memory: 1
      - net.core.somaxconn: 1
      - net.ipv6.conf.all.disable_ipv6: 1
      - net.ipv6.conf.default.disable_ipv6: 1
      - net.ipv6.conf.lo.disable_ipv6: 1
    tags: sysctl
    when: sysctl_result is not success
    ignore_errors: yes

  handlers:
  - name: restart-chrony
    service: name=chrony state=restarted
  - name: restart-sshd
    service: name=sshd state=restarted

#######################################################################
#  .o88b. d88888b d8888b. db   db 
# d8P  Y8 88'     88  `8D 88   88 
# 8P      88ooooo 88oodD' 88ooo88 
# 8b      88~~~~~ 88~~~   88~~~88 
# Y8b  d8 88.     88      88   88 
#  `Y88P' Y88888P 88      YP   YP 
#                                
## Ceph - ssh keys for ceph-deploy (not really used)
#######################################################################
- hosts: control
  tasks:
  # Wont work untill 2.8
  # - name: gen ssh key on control
  #   openssh_keypair:
  #     path: /root/.ssh/id_rsa
  #     type: rsa
  #   tags: ceph

  # Should be replaced with previous step when 2.8 is out
  - name: gen ssh key on control
    shell: ssh-keygen -b 2048 -t rsa -f /root/.ssh/id_rsa -q -N ""
    args:
      creates: /root/.ssh/id_rsa
    tags:
    - ceph
    - ssh-keys

  - name: register privkey
    slurp:
      src: /root/.ssh/id_rsa
    register: privkey
    tags:
    - ceph
    - ssh-keys

  - name: register pubkey
    slurp:
      src: /root/.ssh/id_rsa.pub
    register: pubkey
    tags:
    - ceph
    - ssh-keys

#######################################################################
## Ceph
#######################################################################
- hosts: type_openstack_compute_instance_v2
  tasks:
  - name: distribute ssh keys
    block:
    - name: Set authorized key taken from file for ceph-deploy
      authorized_key:
        user: root
        state: present
        key: "{{ hostvars[item]['pubkey']['content']|b64decode }}"
      with_items: '{{control_node_ip}}'

    - name: Set authorized key taken from file for ceph-deploy
      authorized_key:
        user: ubuntu
        state: present
        key: "{{ hostvars[item]['pubkey']['content']|b64decode }}"
      with_items: '{{control_node_ip}}'
    tags:
    - ceph
    - ssh-keys

# http://docs.ceph.com/docs/mimic/install/manual-deployment/
  - name: create ceph config on all nodes
    block:
      - name: 2. Ensure you have a directory for the Ceph configuration file.
        file:
          path: /etc/ceph
          state: directory
          owner: root
          group: root
          mode: 0755

      - name: 3-7. Create a Ceph configuration file.
        template:
          src=files/ceph/ceph.conf.j2
          dest=/etc/ceph/ceph.conf
          mode=0644
          owner=root
          group=root
    tags: 
      - ceph
      - ceph-config

- hosts: ceph-mon.0
  tasks:
  - name: create keyring with users on first mon
    block:
      - name: 8. Create a keyring for your cluster and secret key.
        shell: |
          ceph-authtool \
          --create-keyring /etc/ceph/tmp_ceph.mon.keyring \
          --gen-key -n mon. \
          --cap mon 'allow *'
        args:
          creates: /etc/ceph/tmp_ceph.mon.keyring
        register: tmpcephmonkeyring

      - name: 9. Gen an administrator keyring and client.admin user.
        shell: |
          ceph-authtool \
          --create-keyring /etc/ceph/ceph.client.admin.keyring \
          --gen-key -n client.admin \
          --set-uid=0 \
          --cap mon 'allow *' \
          --cap osd 'allow *' \
          --cap mds 'allow *' \
          --cap mgr 'allow *'
        args:
          creates: /etc/ceph/ceph.client.admin.keyring
        register: cephadminkeyring

      - name: 10. Gen a bootstrap-osd keyring and client.bootstrap-osd user.
        shell: |
          ceph-authtool \
          --create-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring \
          --gen-key -n client.bootstrap-osd \
          --cap mon 'profile bootstrap-osd'
        args:
          creates: /var/lib/ceph/bootstrap-osd/ceph.keyring
        register: cephbootstrapkeyring

      - name: 11. Add the generated keys to tmp_ceph.mon.keyring.
        shell: |
          ceph-authtool /etc/ceph/tmp_ceph.mon.keyring \
          --import-keyring /etc/ceph/ceph.client.admin.keyring
        when: cephadminkeyring.changed or tmpcephmonkeyring.changed
        
      - name: 11. Add the generated keys to tmp_ceph.mon.keyring.
        shell: |
          ceph-authtool /etc/ceph/tmp_ceph.mon.keyring \
          --import-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring
        when: cephbootstrapkeyring.changed or tmpcephmonkeyring.changed

      - name: 12. Gen a monitor map. Save it as /etc/ceph/tmp_monmap
        shell: |
          monmaptool --create \
          --add {{ ansible_hostname }} {{ access_ip_v4 }} \
          --fsid {{ ceph_fsid }} /etc/ceph/tmp_monmap
        args:
          creates: /etc/ceph/tmp_monmap
        register: monmap

      - name: 12. Add other hosts to monmap
        shell: |
          monmaptool \
          --add {{ hostvars[item].ansible_hostname }} {{ hostvars[item].access_ip_v4 }} \
          --fsid {{ ceph_fsid }} /etc/ceph/tmp_monmap
        loop: "{{ query('inventory_hostnames', 'ceph-mon') }}"
        ignore_errors: true

      - name: 13. Create a default data directory on the monitor host(s).
        file:
          path: /var/lib/ceph/mon/ceph-{{ ansible_hostname }}
          state: directory
          owner: ceph
          group: ceph
          mode: 0755

      - name: 14. Populate the mons with the monmap and keyring.
        shell: |
          sudo -u ceph ceph-mon --cluster ceph \
          --mkfs -i {{ ansible_hostname }} \
          --monmap /etc/ceph/tmp_monmap \
          --keyring /etc/ceph/tmp_ceph.mon.keyring
        args:
          creates: /var/lib/ceph/mon/{{ ansible_hostname }}/store.db
        when: cephbootstrapkeyring.changed or tmpcephmonkeyring.changed or cephadminkeyring.changed

      - name: 16. Touch the done file.
        copy:
          content: ""
          dest: /var/lib/ceph/mon/ceph-{{ ansible_hostname }}/done
          force: no
          owner: ceph
          group: ceph
          mode: 0644

      - name: start ceph-mon
        service: 
          name=ceph-mon@{{ ansible_hostname }}
          state=started

      - name: copy keyring tmp files locally
        fetch: 
          src: /etc/ceph/{{ item }}
          dest: .tmp/
        with_items:
          - tmp_ceph.mon.keyring
          - tmp_monmap
    tags:
      - ceph
      - keyring

- hosts: ceph-mon:!ceph-mon.0
  tasks:
  - name: finish mon install on other nodes
    block:
      - name: copy keyring tmp files
        copy: 
          src: .tmp/{{ groups['ceph-mon'][0] }}/etc/ceph/{{ item }}
          dest: /etc/ceph/
        with_items:
          - tmp_ceph.mon.keyring
          - tmp_monmap
        register: keyringcopy

      - name: 13. Create a default data directory on the monitor host(s).
        file:
          path: /var/lib/ceph/mon/ceph-{{ ansible_hostname }}
          state: directory
          owner: ceph
          group: ceph
          mode: 0755

      - name: 14. Populate the mons with the monmap and keyring.
        become: yes
        become_user: ceph
        shell: |
          ceph-mon --cluster ceph \
          --mkfs -i {{ ansible_hostname }} \
          --monmap /etc/ceph/tmp_monmap \
          --keyring /etc/ceph/tmp_ceph.mon.keyring
        args:
          creates: /var/lib/ceph/mon/{{ ansible_hostname }}/store.db

      - name: 16. Touch the done file.
        copy:
          content: ""
          dest: /var/lib/ceph/mon/ceph-{{ ansible_hostname }}/done
          force: no
          owner: ceph
          group: ceph
          mode: 0644

      - name: start ceph-mon
        service: 
          name=ceph-mon@{{ ansible_hostname }}
          state=started
    tags:
      - ceph
      - keyring

  - name: create manager
    block:
      - name: Create an authentication key for your mgr daemon
        shell: |
          ceph auth get-or-create mgr.{{ ansible_hostname }} mon \
          'allow profile mgr' osd 'allow *' mds 'allow *' \
           -o /var/lib/ceph/mgr/ceph-{{ ansible_hostname }}
        args:
          creates: /var/lib/ceph/mgr/ceph-{{ ansible_hostname }}
        register: mgrauthkey

      - name: start ceph-mgr
        shell: |
          ceph-mgr -i {{ ansible_hostname }}
        when: mgrauthkey.changed
    tags:
      - ceph
      - ceph-mgr


## create the ceph-deploy script
- hosts: control
  tasks:
  - name: template ceph-deploy.sh
    template:
      src=files/ceph/ceph-deploy.sh.j2
      dest=/root/ceph-deploy.sh
      mode=0740
      owner=root
      group=root
    tags: ceph-deploy
    register: cephdeploysh

  - name: zap OSDs and create pools
    shell: /root/ceph-deploy.sh | tee /root/ceph-deploy.log
    tags: ceph
    when: cephdeploysh.changed


#######################################################################
#  .o88b.  .d88b.  d8b   db d888888b d8888b.  .d88b.  db      
# d8P  Y8 .8P  Y8. 888o  88 `~~88~~' 88  `8D .8P  Y8. 88      
# 8P      88    88 88V8o 88    88    88oobY' 88    88 88      
# 8b      88    88 88 V8o88    88    88`8b   88    88 88      
# Y8b  d8 `8b  d8' 88  V888    88    88 `88. `8b  d8' 88booo. 
#  `Y88P'  `Y88P'  VP   V8P    YP    88   YD  `Y88P'  Y88888P 
#                                                       
#                                                           
# d8b   db  .d88b.  d8888b. d88888b .d8888. 
# 888o  88 .8P  Y8. 88  `8D 88'     88'  YP 
# 88V8o 88 88    88 88   88 88ooooo `8bo.   
# 88 V8o88 88    88 88   88 88~~~~~   `Y8b. 
# 88  V888 `8b  d8' 88  .8D 88.     db   8D 
# VP   V8P  `Y88P'  Y8888D' Y88888P `8888Y' 
#
## Control node with Core components:
# Nova, Glance, Horizon, Keystone, Neutron, Cinder, Ceilometer
# Gnocchi
#
## Core components TODO:
# Swift, Heat, Trove, Sahara, Ironic, Zaqar, Manila, Designate, 
# Barbican, Searchlight, Magnum, aodh, cloudkitty, congress, freezer, 
# mistral, monasca-api, monasca-log-api, murano, panko, senlin, solum, 
# tacker, vitrage, Watcher, blazar, ceilometer-powervm, karbor, 
# octavia, storlets, tricircle, zun, Cyborg, ec2-api, Masakari, Qinling

#######################################################################

- hosts: control
  tasks:
    ## Set facts by stripping "metadata." and joining with _
  - name: Set fact from terraform metadata
    set_fact:
      "{{item.key.split('.')[1:10] | join('_') }}": "{{ item.value }}"
    with_dict: "{{ hostvars[inventory_hostname] }}"
    when: 
      - '"metadata" in item.key'
      - '"%" not in item.key'
    tags: always

## Configure haproxy
  - name: template haproxy.cfg
    template:
      src=files/haproxy.cfg.j2
      dest=/etc/haproxy/haproxy.cfg
      mode=0640
      owner=root
      group=root
      backup=yes
      validate='haproxy -c -f %s'
    notify: reload-haproxy
    tags: haproxy
    register: haproxyconfig

  - name: restart haproxy before next steps
    service: 
      name=haproxy 
      state=restarted
    when: haproxyconfig.changed

## MySQL
  - name: template 99-openstack.cnf
    template:
      src=files/99-openstack.cnf.j2
      dest=/etc/mysql/mariadb.conf.d/99-openstack.cnf
      backup=yes
    tags: database
    register: mysqlconfig

  - name: reload mysql before next steps
    service: 
      name=mysql 
      state=restarted
    when: mysqlconfig.changed

  - name: create databases
    mysql_db:
      config_file=/etc/mysql/debian.cnf
      name={{ item }}
      encoding=utf8
      collation=utf8_general_ci
    with_items: "{{mysql_databases}}"
    tags: database

# grant every user all privileges since ansible cant provide multiple 
# grants thats needed for nova_cell0 etc.
  - name: create users
    mysql_user:
      config_file=/etc/mysql/debian.cnf
      name={{item}}
      host='%'
      password="{{ DATABASE_PASS }}"
      priv='*.*:ALL'
      # priv='{{item}}.*:ALL'
    with_items: "{{mysql_databases}}"
    tags: database

## RabbitMQ
  - rabbitmq_user:
      user: guest
      state: absent
    tags: rabbitmq

  - name: Enables the rabbitmq_management plugin
    rabbitmq_plugin:
      names: rabbitmq_management
      state: enabled
    tags: rabbitmq

  - name: add openstack user to rabbitmq
    rabbitmq_user:
      user: openstack
      password: "{{RABBIT_PASS}}"
      vhost: /
      configure_priv: .*
      read_priv: .*
      write_priv: .*
      state: present
    tags: rabbitmq

  - name: add admin user to rabbitmq
    rabbitmq_user:
      user: admin
      password: "{{RABBIT_PASS}}"
      vhost: /
      configure_priv: .*
      read_priv: .*
      write_priv: .*
      tags: administrator
      state: present
    tags: rabbitmq

## Redis
  - name: template redis.conf
    template:
      src=files/redis.conf.j2
      dest=/etc/redis/redis.conf
      backup=yes
    notify: restart-redis
    tags: redis

  - name: chmod /var/lib/redis/ directory
    file:
      path: /var/lib/redis/
      state: directory
      owner: redis
      group: redis
      mode: 0775
    notify: restart-redis
    tags: redis

  # - name: chmod /var/lib/redis/dump.rdb
  #   file:
  #     path: /var/lib/redis/dump.rdb
  #     state: file
  #     owner: redis
  #     group: redis
  #     mode: 0644
  #   notify: restart-redis
  #   tags: redis

## Memcached
  - name: template memcached.conf
    template:
      src=files/memcached.conf.j2
      dest=/etc/memcached.conf
      backup=yes
    notify: restart-memcached

## ETCD
  - name: create etcd config folder
    file:
      path=/etc/etcd
      state=directory
      owner=etcd
      group=etcd

  - name: template etcd.conf.yml
    template:
      src=files/etcd.conf.yml.j2
      dest=/etc/etcd/etcd.conf.yml
      backup=yes
    notify: restart-etcd

## PHPMyAdmin
  - name: enable phpmyadmin
    file:
      src=/etc/phpmyadmin/apache.conf
      dest=/etc/apache2/sites-enabled/phpmyadmin.conf
      state=link
    notify: 
      - restart-apache2
    tags: phpmyadmin

#######################################################################
# db   dD d88888b db    db .d8888. d888888b  .d88b.  d8b   db d88888b 
# 88 ,8P' 88'     `8b  d8' 88'  YP `~~88~~' .8P  Y8. 888o  88 88'     
# 88,8P   88ooooo  `8bd8'  `8bo.      88    88    88 88V8o 88 88ooooo 
# 88`8b   88~~~~~    88      `Y8b.    88    88    88 88 V8o88 88~~~~~ 
# 88 `88. 88.        88    db   8D    88    `8b  d8' 88  V888 88.     
# YP   YD Y88888P    YP    `8888Y'    YP     `Y88P'  VP   V8P Y88888P 
#
## Keystone controller

  - name: install keystone
    apt:  pkg=keystone state=present update_cache=yes
    register: keystone
    tags: keystone

  - name: template keystone.conf.yml
    template:
      src=files/keystone.conf.j2
      dest=/etc/keystone/keystone.conf
      backup=yes
    tags: keystone

  - name: chown /var/log/keystone directory
    file:
      path: /var/log/keystone
      state: directory
      owner: keystone
      group: syslog
      mode: 0775

  - name: retreive keystone table count
    shell: |
      mysql -e "SELECT COUNT(*) FROM information_schema.tables \
      WHERE table_schema = 'keystone';" -B  -N
    register: keystonetablecount
    tags: 
      - keystone
      - firstrun

  - name: Populate the Identity service database
    shell:  su -s /bin/sh -c "keystone-manage db_sync" keystone
    when: keystonetablecount.stdout|int < 10
    tags: 
      - keystone
      - firstrun

  - name: Initialize Fernet key repositories
    shell: |
      keystone-manage fernet_setup --keystone-user keystone \
       --keystone-group keystone
      keystone-manage credential_setup --keystone-user keystone \
       --keystone-group keystone
    register: fernet
    when: keystonetablecount.stdout|int < 10
    tags: 
      - keystone
      - firstrun

  - name: Bootstrap the Identity service
    shell: |
      keystone-manage bootstrap --bootstrap-password {{ ADMIN_PASS }} \
       --bootstrap-admin-url http://{{ control_node_name }}:5000/v3/ \
       --bootstrap-internal-url http://{{ control_node_name }}:5000/v3/ \
       --bootstrap-public-url http://{{ control_node_ip_remote }}:5000/v3/ \
       --bootstrap-region-id RegionOne
    become: yes
    become_user: root
    register: command_result
    failed_when: "'FAILED' in command_result.stderr"
    when: keystonetablecount.stdout|int < 10
    with_items: 
      - "{{ groups['control'] }}"
    tags: 
      - keystone
      - firstrun

  - name: configure keystone domains, roles and users
    block:
      - name: add domains
        os_keystone_domain:
          cloud: "{{ooo}}"
          state: present
          name: "{{item.key}}"
          description: "{{item.value}}"
        with_dict: "{{keystone_domains}}"

      - name: add projects
        os_project:
          cloud: "{{ooo}}"
          name: "{{item.key}}"
          description: "{{item.value}}"
          domain_id: "{{item.value.domain|default('default')}}"
        with_dict: "{{keystone_projects}}"

      - name: add roles to keystone
        os_keystone_role: 
          cloud: "{{ooo}}"
          name: "{{item}}"
        with_items: "{{keystone_roles}}"

      - name: add users to keystone
        os_user: 
          cloud: "{{ooo}}"
          name: "{{item.key}}"
          password: "{{item.value.pass}}"
          update_password: on_create
          domain: "{{item.value.domain|default('default')}}"
        with_dict: "{{keystone_users}}"

      - name: assign roles to users in keystone
        os_user_role: 
          cloud: "{{ooo}}"
          user: "{{item.key}}"
          role: "{{item.value.role}}"
          project: "{{item.value.project}}"
        with_dict: "{{keystone_users}}"

      - name: add domain users to keystone
        os_user: 
          cloud: "{{ooo}}"
          name: "{{item.key}}"
          password: "{{item.value.pass}}"
          update_password: on_create
          domain: "{{item.value.domain|default('default')}}"
        with_dict: "{{keystone_domain_users}}"

      - name: assign roles to domain users in keystone
        os_user_role: 
          cloud: "{{ooo}}"
          user: "{{item.key}}"
          role: "{{item.value.role}}"
          domain: "{{item.value.domain}}"
        with_dict: "{{keystone_domain_users}}"
        ##fails when magnum domain id is not set so ignore errors
        ignore_errors: yes

    tags: 
    - keystone
    - keystone_users

  - name: prep services and endpoints
    block:
      - name: create services in keystone
        os_keystone_service:
          cloud: "{{ooo}}"
          name: "{{item.key}}"
          service_type: "{{item.value.type}}"
          description: "{{item.value.description}}"
        with_dict: "{{keystone_services}}"

      - name: create endpoints in keystone
        os_keystone_endpoint:
          cloud: "{{ooo}}"
          service: "{{item.service}}"
          endpoint_interface: "{{item.interface}}"
          url: "{{item.url}}"
          region: "{{item.region}}"
        with_items: "{{keystone_endpoints}}"
    tags: 
    - keystone
    - keystone_services

  - name: configure Keystone
    template:
      src=files/apache2/sites-available/keystone.conf.j2
      dest=/etc/apache2/sites-available/keystone.conf
      backup=yes
    notify: 
      - restart-apache2



#######################################################################
# db   db d88888b  .d8b.  d888888b 
# 88   88 88'     d8' `8b `~~88~~' 
# 88ooo88 88ooooo 88ooo88    88    
# 88~~~88 88~~~~~ 88~~~88    88    
# 88   88 88.     88   88    88    
# YP   YP Y88888P YP   YP    YP    
#
## HEAT

  - name: install heat
    block:
      - name: configure heat
        template:
          src=files/heat/{{item}}.j2
          dest=/etc/heat/{{item}}
          backup=yes
        with_items:
          - heat.conf
        notify: 
          - restart-heat-api
          - restart-heat-api-cfn
          - restart-heat-engine

      - name: count heat tables
        shell: |
          mysql -e "SELECT COUNT(*) FROM information_schema.tables \
          WHERE table_schema = 'heat';" -B  -N
        register: heattablecount

      - name: Populate heat database
        shell: su -s /bin/sh -c "heat-manage db_sync" heat
        when: heattablecount.stdout|int < 10
        notify: 
          - restart-heat-api
          - restart-heat-api-cfn
          - restart-heat-engine
    tags:
      - heat


#######################################################################
# .88b  d88.  .d8b.   d888b  d8b   db db    db .88b  d88. 
# 88'YbdP`88 d8' `8b 88' Y8b 888o  88 88    88 88'YbdP`88 
# 88  88  88 88ooo88 88      88V8o 88 88    88 88  88  88 
# 88  88  88 88~~~88 88  ooo 88 V8o88 88    88 88  88  88 
# 88  88  88 88   88 88. ~8~ 88  V888 88b  d88 88  88  88 6
# YP  YP  YP YP   YP  Y888P  VP   V8P ~Y8888P' YP  YP  YP 
#
## MAGNUM 

  - name: install magnum
    block:
      - name: configure magnum
        template:
          src=files/magnum/{{item}}.j2
          dest=/etc/magnum/{{item}}
          backup=yes
        with_items:
          - magnum.conf
        notify: 
          - restart-magnum-api
          - restart-magnum-conductor

      - name: count magnum tables
        shell: |
          mysql -e "SELECT COUNT(*) FROM information_schema.tables \
          WHERE table_schema = 'magnum';" -B  -N
        register: magnumtablecount

      - name: Populate magnum database
        shell: su -s /bin/sh -c "magnum-db-manage upgrade" magnum
        when: magnumtablecount.stdout|int < 5
        notify: 
          - restart-magnum-api
          - restart-magnum-conductor
    tags:
      - magnum


#######################################################################
#  d888b  db       .d8b.  d8b   db  .o88b. d88888b 
# 88' Y8b 88      d8' `8b 888o  88 d8P  Y8 88'     
# 88      88      88ooo88 88V8o 88 8P      88ooooo 
# 88  ooo 88      88~~~88 88 V8o88 8b      88~~~~~ 
# 88. ~8~ 88booo. 88   88 88  V888 Y8b  d8 88.     
#  Y888P  Y88888P YP   YP VP   V8P  `Y88P' Y88888P                                                
#
## Glance

  - name: install glance
    block:
      - name: configure glance
        template:
          src=files/{{item}}.j2
          dest=/etc/glance/{{item}}
          backup=yes
        with_items:
          - glance-api.conf
          - glance-registry.conf
        notify: 
          - restart-glance-api
          - restart-glance-registry

      - name: count glance tables
        shell: |
          mysql -e "SELECT COUNT(*) FROM information_schema.tables \
          WHERE table_schema = 'glance';" -B  -N
        register: glancetablecount

      - name: Populate glance database
        shell: su -s /bin/sh -c "glance-manage db_sync" glance
        when: glancetablecount.stdout|int < 10
        notify: 
          - restart-glance-api
          - restart-glance-registry
    tags:
      - glance


#######################################################################
# d8b   db  .d88b.  db    db  .d8b.  
# 888o  88 .8P  Y8. 88    88 d8' `8b 
# 88V8o 88 88    88 Y8    8P 88ooo88 
# 88 V8o88 88    88 `8b  d8' 88~~~88 
# 88  V888 `8b  d8'  `8bd8'  88   88 
# VP   V8P  `Y88P'     YP    YP   YP 
#
## Nova controller

  - name: configure nova
    block:
      - name: configure nova
        template:
          src=files/{{item}}.j2
          dest=/etc/{{item}}
          backup=yes
        with_items:
          - nova/nova.conf
        notify: 
          - restart-nova-api
          - restart-nova-conductor
          - restart-nova-scheduler
          - restart-nova-novncproxy
          # - restart-nova-spiceproxy

      - name: count nova api tables
        shell: |
          mysql -e "SELECT COUNT(*) FROM information_schema.tables \
          WHERE table_schema = 'nova_api';" -B  -N
        register: novaapitablecount

      - name: Populate the Nova API database
        shell: su -s /bin/sh -c "nova-manage api_db sync" nova
        when: novaapitablecount.stdout|int < 10

      - name: count nova tables
        shell: |
          mysql -e "SELECT COUNT(*) FROM information_schema.tables \
          WHERE table_schema = 'nova';" -B  -N
        register: novatablecount

      - name: stop nova before populating database
        service:
          name: "{{item}}"
          state: stopped
        with_items:
          - nova-api
          - nova-conductor
          - nova-scheduler
          - nova-novncproxy
        when: novatablecount.stdout|int < 10

      - name: Populate nova databases
        shell: |
          su -s /bin/sh -c "nova-manage cell_v2 map_cell0" nova
          su -s /bin/sh -c "nova-manage cell_v2 create_cell \
           --name=cell1 \
           --verbose" nova 109e1d4b-536a-40d0-83c6-5f121b82b650
          su -s /bin/sh -c "nova-manage db sync" nova
        when: novatablecount.stdout|int < 10
        notify: 
          - restart-nova-api
          - restart-nova-conductor
          - restart-nova-scheduler
          - restart-nova-novncproxy
          # - restart-nova-spiceproxy
    tags: nova

#######################################################################
#  .o88b. d888888b d8b   db d8888b. d88888b d8888b. 
# d8P  Y8   `88'   888o  88 88  `8D 88'     88  `8D 
# 8P         88    88V8o 88 88   88 88ooooo 88oobY' 
# 8b         88    88 V8o88 88   88 88~~~~~ 88`8b   
# Y8b  d8   .88.   88  V888 88  .8D 88.     88 `88. 
#  `Y88P' Y888888P VP   V8P Y8888D' Y88888P 88   YD 
#
## Cinder controller

  - name: configure cinder
    block:
      - name: configure cinder
        template:
          src=files/{{item}}.j2
          dest=/etc/{{item}}
          backup=yes
        with_items:
          - cinder/cinder.conf
        notify: 
          - restart-cinder-scheduler
          - restart-apache2
          - restart-cinder-volume
          - restart-cinder-backup

      - name: create cinder stats cronjob
        cron:
          name="check dirs"
          minute="*/5"
          hour="*"
          job="/usr/bin/cinder-volume-usage-audit --send_actions"
        notify: 
          - restart-cinder-scheduler
          - restart-apache2

      - name: count cinder tables
        shell: |
          mysql -e "SELECT COUNT(*) FROM information_schema.tables \
          WHERE table_schema = 'cinder';" -B  -N
        register: cindertablecount

      - name: Populate the cinder database
        shell: su -s /bin/sh -c "cinder-manage db sync" cinder
        when: cindertablecount.stdout|int < 10
        register: cinderconfig

      - name: reload cinder before next steps
        service: 
          name=cinder-volume
          state=restarted
        when: cinderconfig.changed
    tags: cinder


      # - name: 

#######################################################################
# d8b   db d88888b db    db d888888b d8888b.  .d88b.  d8b   db 
# 888o  88 88'     88    88 `~~88~~' 88  `8D .8P  Y8. 888o  88 
# 88V8o 88 88ooooo 88    88    88    88oobY' 88    88 88V8o 88 
# 88 V8o88 88~~~~~ 88    88    88    88`8b   88    88 88 V8o88 
# 88  V888 88.     88b  d88    88    88 `88. `8b  d8' 88  V888 
# VP   V8P Y88888P ~Y8888P'    YP    88   YD  `Y88P'  VP   V8P 
#
## Neutron controller

  - name: configure neutron controller
    block:
    - name: configure neutron
      template:
        src=files/{{item}}.j2
        dest=/etc/{{item}}
        backup=yes
      with_items:
        - neutron/neutron.conf
        - neutron/plugins/ml2/ml2_conf.ini
#        - neutron/plugins/ml2/linuxbridge_agent.ini
        - neutron/l3_agent.ini
        - neutron/dhcp_agent.ini
        - neutron/metadata_agent.ini
      notify: 
        - restart-nova-api
        - restart-neutron-server
        - restart-neutron-linuxbridge-agent
        - restart-neutron-dhcp-agent
        - restart-neutron-metadata-agent
        - restart-neutron-l3-agent
    tags: neutron

  - name: retreive neutron table count
    shell: |
      mysql -e "SELECT COUNT(*) FROM information_schema.tables \
      WHERE table_schema = 'neutron';" -B  -N
    register: neutrontablecount
    tags: neutron

  - name: Populate the Identity service database
    shell: |
      su -s /bin/sh -c "neutron-db-manage \
      --config-file /etc/neutron/neutron.conf \
      --config-file /etc/neutron/plugins/ml2/ml2_conf.ini \
      upgrade head" neutron
    when: neutrontablecount.stdout|int < 10
    tags: neutron

  - name: ensure br_netfilter kernel module is available
    modprobe:
      name: br_netfilter
      state: present
    tags: neutron


#######################################################################
# db   db  .d88b.  d8888b. d888888b d88888D  .d88b.  d8b   db 
# 88   88 .8P  Y8. 88  `8D   `88'   YP  d8' .8P  Y8. 888o  88 
# 88ooo88 88    88 88oobY'    88       d8'  88    88 88V8o 88 
# 88~~~88 88    88 88`8b      88      d8'   88    88 88 V8o88 
# 88   88 `8b  d8' 88 `88.   .88.    d8' db `8b  d8' 88  V888 
# YP   YP  `Y88P'  88   YD Y888888P d88888P  `Y88P'  VP   V8P 
#
## Openstack-dashboard

  - name: configure Openstack-dashboard
    template:
      src=files/openstack-dashboard/local_settings.py.j2
      dest=/etc/openstack-dashboard/local_settings.py
      backup=yes
    notify: 
      - restart-apache2

  handlers:
  - name: restart-heat-api
    service: name=heat-api state=restarted
  - name: restart-heat-api-cfn
    service: name=heat-api-cfn state=restarted
  - name: restart-heat-engine
    service: name=heat-engine state=restarted
  - name: reload-haproxy
    service: name=haproxy state=reloaded
  - name: restart-mysql
    service: name=mysql state=restarted
  - name: restart-memcached
    service: name=memcached state=restarted
  - name: restart-redis
    service: name=redis state=restarted
  - name: restart-etcd
    service: name=etcd state=restarted
  - name: restart-apache2
    service: name=apache2 state=restarted

  - name: restart-cinder-backup
    service: name="cinder-backup" state=restarted
  - name: restart-cinder-scheduler
    service: name="cinder-scheduler" state=restarted
  - name: restart-cinder-volume
    service: name="cinder-volume" state=restarted

  - name: restart-glance-registry
    service: name=glance-registry state=restarted
  - name: restart-glance-api
    service: name=glance-api state=restarted

  - name: restart-magnum-conductor
    service: name=magnum-conductor state=restarted
  - name: restart-magnum-api
    service: name=magnum-api state=restarted

  - name: restart-nova-api
    service: name="nova-api" state=restarted
  - name: restart-nova-conductor
    service: name="nova-conductor" state=restarted
  - name: restart-nova-scheduler
    service: name="nova-scheduler" state=restarted
  - name: restart-nova-novncproxy
    service: name="nova-novncproxy" state=restarted

  - name: restart-neutron-server
    service: name="neutron-server" state=restarted
  - name: restart-neutron-linuxbridge-agent
    service: name="neutron-linuxbridge-agent" state=restarted
  - name: restart-neutron-dhcp-agent
    service: name="neutron-dhcp-agent" state=restarted
  - name: restart-neutron-metadata-agent
    service: name="neutron-metadata-agent" state=restarted
  - name: restart-neutron-l3-agent
    service: name="neutron-l3-agent" state=restarted

#######################################################################
#  d888b  d8b   db  .d88b.   .o88b.  .o88b. db   db d888888b 
# 88' Y8b 888o  88 .8P  Y8. d8P  Y8 d8P  Y8 88   88   `88'   
# 88      88V8o 88 88    88 8P      8P      88ooo88    88    
# 88  ooo 88 V8o88 88    88 8b      8b      88~~~88    88    
# 88. ~8~ 88  V888 `8b  d8' Y8b  d8 Y8b  d8 88   88   .88.   
#  Y888P  VP   V8P  `Y88P'   `Y88P'  `Y88P' YP   YP Y888888P 
#
## Gnocchi
#######################################################################

- hosts: gnocchi-api
  tasks:
  - name: chown /var/lib/gnocchi directory
    file:
      path: /var/lib/gnocchi/tmp
      state: directory
      owner: gnocchi
      group: gnocchi
      mode: 0775
  
  - name: configure gnocchi
    template:
      src=files/{{item}}.j2
      dest=/etc/{{item}}
      backup=yes
    with_items:
      - gnocchi/gnocchi.conf
      - gnocchi/gnocchi_resources.yaml
    notify: 
      - init-gnocchi
      - restart-apache2
    tags: gnocchi

  handlers:
  - name: restart-apache2
    service: name=apache2 state=restarted
  - name: init-gnocchi
    shell: gnocchi-upgrade
  
- hosts: gnocchi-metricd
  tasks:
  - name: chown /var/lib/gnocchi directory
    file:
      path: /var/lib/gnocchi/tmp
      state: directory
      owner: gnocchi
      group: gnocchi
      mode: 0775
  
  - name: configure gnocchi
    template:
      src=files/{{item}}.j2
      dest=/etc/{{item}}
      backup=yes
    with_items:
      - gnocchi/gnocchi.conf
      - gnocchi/gnocchi_resources.yaml
    notify: 
      - init-gnocchi
      - restart-gnocchi-metricd
    tags: gnocchi

  handlers:
  - name: init-gnocchi
    shell: gnocchi-upgrade
  - name: restart-gnocchi-metricd
    service: name="gnocchi-metricd" state=restarted

#######################################################################
#  .o88b. d88888b d888888b db       .d88b.  .88b  d88. d88888b d888888b d88888b d8888b.
# d8P  Y8 88'       `88'   88      .8P  Y8. 88'YbdP`88 88'     `~~88~~' 88'     88  `8D 
# 8P      88ooooo    88    88      88    88 88  88  88 88ooooo    88    88ooooo 88oobY' 
# 8b      88~~~~~    88    88      88    88 88  88  88 88~~~~~    88    88~~~~~ 88`8b   
# Y8b  d8 88.       .88.   88booo. `8b  d8' 88  88  88 88.        88    88.     88 `88. 
#  `Y88P' Y88888P Y888888P Y88888P  `Y88P'  YP  YP  YP Y88888P    YP    Y88888P 88   YD 
#
## Ceilometer

- hosts: control
  tasks:
  - name: configure ceilometer
    template:
      src=files/{{item}}.j2
      dest=/etc/{{item}}
      backup=yes
    with_items:
      - ceilometer/ceilometer.conf
      - ceilometer/pipeline.yaml
      - ceilometer/event_definitions.yaml
      - ceilometer/event_pipeline.yaml
    notify: 
      - init-ceilometer
      - restart-ceilometer-agent-central
      - restart-ceilometer-agent-notification
    tags: ceilometer

  handlers:
  - name: init-ceilometer
    shell: ceilometer-upgrade
  - name: restart-ceilometer-agent-central
    service: name="ceilometer-agent-central" state=restarted
  - name: restart-ceilometer-agent-notification
    service: name="ceilometer-agent-notification" state=restarted

#######################################################################
#  .o88b.  .d88b.  .88b  d88. d8888b. db    db d888888b d88888b 
# d8P  Y8 .8P  Y8. 88'YbdP`88 88  `8D 88    88 `~~88~~' 88'     
# 8P      88    88 88  88  88 88oodD' 88    88    88    88ooooo 
# 8b      88    88 88  88  88 88~~~   88    88    88    88~~~~~ 
# Y8b  d8 `8b  d8' 88  88  88 88      88b  d88    88    88.     
#  `Y88P'  `Y88P'  YP  YP  YP 88      ~Y8888P'    YP    Y88888P 
#                                                              
#                                                              
# d8b   db  .d88b.  d8888b. d88888b .d8888. 
# 888o  88 .8P  Y8. 88  `8D 88'     88'  YP 
# 88V8o 88 88    88 88   88 88ooooo `8bo.   
# 88 V8o88 88    88 88   88 88~~~~~   `Y8b. 
# 88  V888 `8b  d8' 88  .8D 88.     db   8D 
# VP   V8P  `Y88P'  Y8888D' Y88888P `8888Y' 
#                                          
## Compute nodes
#######################################################################

- hosts: compute
  tasks:
  - name: distribute ssh keys
    block:
    - name: Set authorized key taken from file for ceph-deploy
      authorized_key:
        user: nova
        state: present
        key: "{{ hostvars[item]['pubkey']['content']|b64decode }}"
      with_items: '{{control_node_ip}}'

    - name: create nova home
      file:
        path: /var/lib/nova/.ssh
        state: directory
        owner: nova
        group: nova
        mode: 0700
    
    - name: Set privkey taken from file on ceph-deploy
      copy:
        dest: /var/lib/nova/.ssh/id_rsa
        owner: nova
        group: nova
        mode: 0700
        checksum: false
        content: "{{ hostvars[item]['privkey']['content']|b64decode }}"
      with_items: '{{control_node_ip}}'
    tags:
    - ssh-keys

## Configure ceilometer
  - name: configure ceilometer
    template:
      src=files/{{item}}.j2
      dest=/etc/{{item}}
      backup=yes
    with_items:
      - ceilometer/ceilometer.conf
#      - ceilometer/compute_pipeline.yaml
    notify: 
      - restart-ceilometer-agent-compute
      - restart-nova-compute
    tags:
      - ceilometer
      - compute-node

## Configure nova & neutron
  - name: configure nova & neutron
    template:
      src=files/{{item}}.j2
      dest=/etc/{{item}}
      backup=yes
    with_items:
      - nova/nova.conf
      - nova/nova-compute.conf
      - neutron/neutron.conf
      - neutron/plugins/ml2/linuxbridge_agent.ini
    notify: 
      - restart-neutron-linuxbridge-agent
      - restart-nova-compute
    tags:
      - compute-node
      - nova
      - neutron

  - name: ensure br_netfilter kernel module is available
    modprobe:
      name: br_netfilter
      state: present
    tags: compute-node

  handlers:
  - name: restart-ceilometer-agent-compute
    service: name=ceilometer-agent-compute state=restarted
  # - name: restart-ceilometer-agent-ipmi
  #   service: name=ceilometer-agent-ipmi state=restarted
  - name: restart-neutron-linuxbridge-agent
    service: name=neutron-linuxbridge-agent state=restarted
  - name: restart-nova-compute
    service: name=nova-compute state=restarted


#######################################################################
#      d88888b d888888b d8b   db d888888b .d8888. db   db 
#      88'       `88'   888o  88   `88'   88'  YP 88   88 
#      88ooo      88    88V8o 88    88    `8bo.   88ooo88 
#      88~~~      88    88 V8o88    88      `Y8b. 88~~~88 
#      88        .88.   88  V888   .88.   db   8D 88   88 
#      YP      Y888888P VP   V8P Y888888P `8888Y' YP   YP 
## Finish up control nodes (
## Use hosts: control (internal ip) because thats where the metadata is
#######################################################################

- hosts: control
  tasks:
  - name: make sure all services are started
    service: 
      name={{item}} 
      state=started
    with_items:
      - apache2 
      - etcd 
      - glance-api 
      - glance-registry 
      - memcached 
      - mysql 
      - "neutron-dhcp-agent" 
      - "neutron-l3-agent" 
      # - "neutron-linuxbridge-agent" 
      - "neutron-metadata-agent" 
      - "neutron-server" 
      - "nova-api" 
      - "nova-conductor" 
      - "nova-novncproxy" 
      - "nova-scheduler" 
    tags: finalize

  - name: add compute nodes to cell database
    shell: |
      . ~/adminrc && su -s /bin/sh -c "nova-manage cell_v2 \
      discover_hosts --verbose" nova
    tags:
      - compute
      - firstrun

  ### werkt niet om een of andere reden, endpoint not found?
  # - name: Create tiny flavor
  #   os_nova_flavor:
  #     cloud: "{{ooo}}"
  #     state: present
  #     name: tiny3
  #     ram: 512
  #     vcpus: 1
  #     ephemeral: 10
  #     disk: 10
  #     endpoint_type: public
  #   tags: finalize

  - name: add flavor with salt and pepper
    shell: |
      . ~/adminrc && openstack flavor create --ram 512 --disk 10 \
       --vcpus 1 --public m1.small || true
    tags:
      - flavor

  - name: Create a net-public
    os_network:
      cloud: "{{ooo}}"
#      state: present
      name: net-public
      project: admin
      provider_network_type: flat
      provider_physical_network: provider
#      provider_segmentation_id:
      shared: true
      external: true
      port_security_enabled: no
    tags: finalize

  - name: Create subnet-public
    os_subnet:
      cloud: "{{ooo}}"
      name: subnet-public
      network_name: "net-public"
#      host_routes: 
      gateway_ip: "{{ pub_gw }}"
      # allocation_pool_start:
      # allocation_pool_end:
      cidr: "{{ pub_cidr }}"

      # dns_nameservers:
    tags: finalize

  - name: download cirros image
    get_url:
      url: http://download.cirros-cloud.net/0.4.0/cirros-0.4.0-x86_64-disk.img
      dest: /tmp/cirros-0.4.0-x86_64-disk.img
    tags: finalize

  - name: upload cirros image
    os_image:
      cloud: "{{ooo}}"
      name: cirros
      container_format: bare
      disk_format: qcow2
      state: present
      filename: /tmp/cirros-0.4.0-x86_64-disk.img
      kernel: cirros-vmlinuz
      ramdisk: cirros-initrd
      properties:
        cpu_arch: x86_64
        distro: cirros
    tags: finalize

  - name: get openstack services
    shell: |
      . ~/adminrc 
      openstack image list
      openstack domain list
      openstack project list
      openstack user list
      openstack compute service list
      openstack image list
      openstack endpoint list --sort-column "Service Name" \
       --sort-column "Interface"
      echo "rabbitmq_management http://81.4.84.46:15672"
    register: show_result
    tags: 
      - show_result
      - finalize

  - debug: var=show_result.stdout_lines
    tags: 
      - show_result
      - finalize

  - debug: 
      msg:
      - "domain          = default"
      - "admin user pass = {{ ADMIN_PASS }}"
      - "myuser user pass= {{ MYUSER_PASS }}"
      - "demo user pass  = {{ DEMO_PASS }}"
      - "horizon url     = http://{{control_node_ip_remote}}/horizon"
      - "haproxy stats   = http://{{control_node_ip_remote}}:8001/stats"
      - "PHPMyAdmin      = http://{{control_node_ip_remote}}/phpmyadmin"
    tags: 
      - show_result
      - finalize
