## OpenStack on OpenStack lab setup
## Author: Cees Moerkerken
## 
## Will create a virtual openstack setup on top of openstack
## Only intended for testing purposes!!!
##
## Run as
## TF_STATE=./tf_openstack/terraform.tfstate ansible-playbook os_lab.yml"
## When using a vault add: "--ask-vault-pass"

---

## install python via raw module
- hosts: compute, control, gnocchi-api
  gather_facts: False

  tasks:
  - name: install python 2
    raw: |
      test -e /usr/bin/python || \
       (sudo apt -y update && sudo apt install -y python-minimal)
    tags: firstrun

#######################################################################
## common stuff
#######################################################################
- hosts: all
  gather_facts: True
  tasks:
  - name: "Ansible setup"
    setup:
    tags: always

- hosts: compute, control, gnocchi-api
  tasks:
  - name: "Build hosts file"
    lineinfile: 
      dest=/etc/hosts 
      regexp='{{ hostvars[item].access_ip_v4 }}.*' 
      line="{{ hostvars[item].access_ip_v4 }} {{ hostvars[item].ansible_hostname }}" 
      state=present
    with_items: 
      - "{{ groups['compute'] }}"
      - "{{ groups['control'] }}"
      - "{{ groups['gnocchi-api'] }}"
    tags: 
      - firstrun
      - hostsfile

  - name: add-apt-repository
    shell: |
      add-apt-repository cloud-archive:rocky -y; apt-key adv \
       --keyserver keyserver.ubuntu.com --recv-keys 5EDB1B62EC4926EA
    tags: firstrun

#  - name: add-apt-repository rocky
#    apt_repository:
#      repo: cloud-archive:rocky

  - name: dist-upgrade
    apt:
      upgrade: dist
      update_cache: yes
      dpkg_options: 'force-confold,force-confdef'
    tags: firstrun
    ##retry so we can use forking
    register: apt_result
    retries: 10
    until: apt_result is success

  - name: ensure common packages are installed
    apt:
      pkg:
        - chrony
        - software-properties-common
        - python-openstackclient 
      state: present
      update_cache: yes
    ##retry so we can use forking
    register: apt_result
    retries: 10
    until: apt_result is success

  - name: configure chrony
    lineinfile:
      path=/etc/chrony/chrony.conf
      regexp='^allow'
      line='allow 10.0.0.0/24'
    notify: restart-chrony

  - name: configure vim (syntax highligt .conf files as ini)
    lineinfile:
      path=/etc/vim/vimrc
      line='autocmd BufRead,BufNewFile *.conf setf dosini'

  - name: set timezone
    timezone:
      name: Europe/Bratislava

  - name: template rc files on all nodes (might come in handy)
    template:
      src=files/{{item}}.j2
      dest=/home/ubuntu/{{item}}
      backup=yes
    with_items:
      - adminrc
      - userrc
      - demorc
    tags: rc

  - name: add adminrc to bashrc on all nodes
    lineinfile:
      path={{item}}
      line='. ~/adminrc'
    with_items:
      - /home/ubuntu/.bashrc
    tags: rc

  - name: configure sysctl
    sysctl:
      name: "{{item.key}}"
      value: "{{item.value}}"
    with_dict:
      - net.bridge.bridge-nf-call-iptables: 1
      - net.bridge.bridge-nf-call-ip6tables: 1
      - vm.overcommit_memory: 1
      - net.core.somaxconn: 1
    tags: sysctl
    ##fails and retry doesnt work, so ignore errors
    ignore_errors: yes

    ##and try again the ugly way
  - name: configure sysctl
    sysctl:
      name: "{{item.key}}"
      value: "{{item.value}}"
    with_dict:
      - net.bridge.bridge-nf-call-iptables: 1
      - net.bridge.bridge-nf-call-ip6tables: 1
      - vm.overcommit_memory: 1
      - net.core.somaxconn: 1
    tags: sysctl
    ignore_errors: yes

  handlers:
  - name: restart-chrony
    service: name=chrony state=restarted

#######################################################################
## Control nodes
#######################################################################
- hosts: control
  tasks:
    ## Set facts by stripping "metadata." and joining with _ to replace dots
  - name: Set fact from terraform metadata
    set_fact:
      "{{item.key.split('.')[1:10] | join('_') }}": "{{ item.value }}"
    with_dict: "{{ hostvars[inventory_hostname] }}"
    when: 
      - '"metadata" in item.key'
      - '"%" not in item.key'
    tags: always

## pakages
  - name: ensure certain packages are installed on control
    apt:  pkg={{packages}} state=present update_cache=yes
    tags: packages

## Configure haproxy
  - name: template haproxy.cfg
    template:
      src=files/haproxy.cfg.j2
      dest=/etc/haproxy/haproxy.cfg
      mode=0640
      owner=root
      group=root
      backup=yes
      validate='haproxy -c -f %s'
    notify: reload-haproxy
    tags: haproxy


## MySQL
  - name: template 99-openstack.cnf
    template:
      src=files/99-openstack.cnf.j2
      dest=/etc/mysql/mariadb.conf.d/99-openstack.cnf
      backup=yes
    tags: database
    register: mysqlconfig

  - name: reload mysql before next steps
    service: 
      name=mysql 
      state=restarted
    when: mysqlconfig.changed

  - name: create databases
    mysql_db:
      config_file=/etc/mysql/debian.cnf
      name={{ item }}
      encoding=utf8
      collation=utf8_general_ci
    with_items: "{{mysql_databases}}"
    tags: database

# grant every user all privileges since ansible cant provide multiple grants thats needed for nova_cell0 etc.
  - name: create users
    mysql_user:
      config_file=/etc/mysql/debian.cnf
      name={{item}}
      host='%'
      password="{{ DATABASE_PASS }}"
      priv='*.*:ALL'
      # priv='{{item}}.*:ALL'
    with_items: "{{mysql_databases}}"
    tags: database

## RabbitMQ
  - rabbitmq_user:
      user: guest
      state: absent
    tags: rabbitmq

  - name: Enables the rabbitmq_management plugin
    rabbitmq_plugin:
      names: rabbitmq_management
      state: enabled
    tags: rabbitmq

  - name: add openstack user to rabbitmq
    rabbitmq_user:
      user: openstack
      password: "{{RABBIT_PASS}}"
      vhost: /
      configure_priv: .*
      read_priv: .*
      write_priv: .*
      state: present
    tags: rabbitmq

  - name: add admin user to rabbitmq
    rabbitmq_user:
      user: admin
      password: "{{RABBIT_PASS}}"
      vhost: /
      configure_priv: .*
      read_priv: .*
      write_priv: .*
      tags: administrator
      state: present
    tags: rabbitmq

## Redis
  - name: template redis.conf
    template:
      src=files/redis.conf.j2
      dest=/etc/redis/redis.conf
      backup=yes
    notify: restart-redis
    tags: redis

  - name: chmod /var/lib/redis/ directory
    file:
      path: /var/lib/redis/
      state: directory
      owner: redis
      group: redis
      mode: 0775
    notify: restart-redis
    tags: redis

  - name: chmod /var/lib/redis/dump.rdb
    file:
      path: /var/lib/redis/dump.rdb
      state: file
      owner: redis
      group: redis
      mode: 0644
    notify: restart-redis
    tags: redis

## Memcached
  - name: template memcached.conf
    template:
      src=files/memcached.conf.j2
      dest=/etc/memcached.conf
      backup=yes
    notify: restart-memcached

## ETCD
  - name: create etcd config folder
    file:
      path=/etc/etcd
      state=directory
      owner=etcd
      group=etcd

  - name: template etcd.conf.yml
    template:
      src=files/etcd.conf.yml.j2
      dest=/etc/etcd/etcd.conf.yml
      backup=yes
    notify: restart-etcd

#######################################################################
## Keystone
  - name: install keystone
    apt:  pkg=keystone state=present update_cache=yes
    register: keystone
    tags: keystone

  - name: template keystone.conf.yml
    template:
      src=files/keystone.conf.j2
      dest=/etc/keystone/keystone.conf
      backup=yes
    tags: keystone

  - name: chown /var/log/keystone directory
    file:
      path: /var/log/keystone
      state: directory
      owner: keystone
      group: syslog
      mode: 0775

  - name: retreive keystone table count
    shell: |
      mysql -e "SELECT COUNT(*) FROM information_schema.tables \
      WHERE table_schema = 'keystone';" -B  -N
    register: keystonetablecount
    tags: 
      - keystone
      - firstrun

  - name: Populate the Identity service database
    shell:  su -s /bin/sh -c "keystone-manage db_sync" keystone
    when: keystonetablecount.stdout|int < 10
    tags: 
      - keystone
      - firstrun

  - name: Initialize Fernet key repositories
    shell: |
      keystone-manage fernet_setup --keystone-user keystone \
       --keystone-group keystone
      keystone-manage credential_setup --keystone-user keystone \
       --keystone-group keystone
    register: fernet
    when: keystonetablecount.stdout|int < 10
    tags: 
      - keystone
      - firstrun

  - name: Bootstrap the Identity service
    shell: |
      keystone-manage bootstrap --bootstrap-password {{ ADMIN_PASS }} \
       --bootstrap-admin-url http://{{ control_node_name }}:5000/v3/ \
       --bootstrap-internal-url http://{{ control_node_name }}:5000/v3/ \
       --bootstrap-public-url http://{{ control_node_ip_remote }}:5000/v3/ \
       --bootstrap-region-id RegionOne
    become: yes
    become_user: root
    failed_when: "'FAILED' in command_result.stderr"
    when: keystonetablecount.stdout|int < 10
    with_items: 
      - "{{ groups['control'] }}"
    tags: 
      - keystone
      - firstrun

  - name: configure keystone domains, roles and users
    block:
      - name: add domains
        os_keystone_domain:
          cloud: "{{ooo}}"
          state: present
          name: "{{item.key}}"
          description: "{{item.value}}"
        with_dict: "{{keystone_domains}}"

      - name: add projects
        os_project:
          cloud: "{{ooo}}"
          name: "{{item.key}}"
          description: "{{item.value}}"
          domain_id: default
        with_dict: "{{keystone_projects}}"

      - name: add roles to keystone
        os_keystone_role: 
          cloud: "{{ooo}}"
          name: "{{item}}"
        with_items: "{{keystone_roles}}"

      - name: add users to keystone
        os_user: 
          cloud: "{{ooo}}"
          name: "{{item.key}}"
          password: "{{item.value.pass}}"
          update_password: on_create
          domain: default
        with_dict: "{{keystone_users}}"

      - name: assign roles to users in keystone
        os_user_role: 
          cloud: "{{ooo}}"
          user: "{{item.key}}"
          role: "{{item.value.role}}"
          project: "{{item.value.project}}"
        with_dict: "{{keystone_users}}"
    tags: 
    - keystone
    - keystone_users

  - name: prep services and endpoints
    block:
      - name: create services in keystone
        os_keystone_service:
          cloud: "{{ooo}}"
          name: "{{item.key}}"
          service_type: "{{item.value.type}}"
          description: "{{item.value.description}}"
        with_dict: "{{keystone_services}}"

      - name: create endpoints in keystone
        os_keystone_endpoint:
          cloud: "{{ooo}}"
          service: "{{item.service}}"
          endpoint_interface: "{{item.interface}}"
          url: "{{item.url}}"
          region: "{{item.region}}"
        with_items: "{{keystone_endpoints}}"
    tags: 
    - keystone
    - keystone_services


#######################################################################
## Glance
  - name: install glance
    block:
      - name: configure glance
        template:
          src=files/{{item}}.j2
          dest=/etc/glance/{{item}}
          backup=yes
        with_items:
          - glance-api.conf
          - glance-registry.conf
        notify: 
          - restart-glance-api
          - restart-glance-registry

      - name: count glance tables
        shell: |
          mysql -e "SELECT COUNT(*) FROM information_schema.tables \
          WHERE table_schema = 'glance';" -B  -N
        register: glancetablecount

      - name: Populate glance database
        shell: su -s /bin/sh -c "glance-manage db_sync" glance
        when: glancetablecount.stdout|int < 10
        notify: 
          - restart-glance-api
          - restart-glance-registry
    tags: glance


#######################################################################
## Nova controller
  - name: configure nova
    block:
      - name: configure nova
        template:
          src=files/{{item}}.j2
          dest=/etc/{{item}}
          backup=yes
        with_items:
          - nova/nova.conf
        notify: 
          - restart-nova-api
          - restart-nova-conductor
          - restart-nova-scheduler
          - restart-nova-novncproxy
          # - restart-nova-spiceproxy

      - name: count nova api tables
        shell: |
          mysql -e "SELECT COUNT(*) FROM information_schema.tables \
          WHERE table_schema = 'nova_api';" -B  -N
        register: novaapitablecount

      - name: Populate the Nova API database
        shell: su -s /bin/sh -c "nova-manage api_db sync" nova
        when: novaapitablecount.stdout|int < 10

      - name: count nova tables
        shell: |
          mysql -e "SELECT COUNT(*) FROM information_schema.tables \
          WHERE table_schema = 'nova';" -B  -N
        register: novatablecount

      - name: stop nova before populating database
        service:
          name: "{{item}}"
          state: stopped
        with_items:
          - nova-api
          - nova-conductor
          - nova-scheduler
          - nova-novncproxy
        when: novatablecount.stdout|int < 10

      - name: Populate nova databases
        shell: |
          su -s /bin/sh -c "nova-manage cell_v2 map_cell0" nova
          su -s /bin/sh -c "nova-manage cell_v2 create_cell \
           --name=cell1 \
           --verbose" nova 109e1d4b-536a-40d0-83c6-5f121b82b650
          su -s /bin/sh -c "nova-manage db sync" nova
        when: novatablecount.stdout|int < 10
        notify: 
          - restart-nova-api
          - restart-nova-conductor
          - restart-nova-scheduler
          - restart-nova-novncproxy
          # - restart-nova-spiceproxy
    tags: nova


#######################################################################
## Neutron controller
  - name: configure neutron controller
    block:
    - name: configure neutron
      template:
        src=files/{{item}}.j2
        dest=/etc/{{item}}
        backup=yes
      with_items:
        - neutron/neutron.conf
        - neutron/plugins/ml2/ml2_conf.ini
#        - neutron/plugins/ml2/linuxbridge_agent.ini
        - neutron/l3_agent.ini
        - neutron/dhcp_agent.ini
        - neutron/metadata_agent.ini
      notify: 
        - restart-nova-api
        - restart-neutron-server
        - restart-neutron-linuxbridge-agent
        - restart-neutron-dhcp-agent
        - restart-neutron-metadata-agent
        - restart-neutron-l3-agent
    tags: neutron

  - name: retreive neutron table count
    shell: |
      mysql -e "SELECT COUNT(*) FROM information_schema.tables \
      WHERE table_schema = 'neutron';" -B  -N
    register: neutrontablecount
    tags: neutron

  - name: Populate the Identity service database
    shell: |
      su -s /bin/sh -c "neutron-db-manage \
      --config-file /etc/neutron/neutron.conf \
      --config-file /etc/neutron/plugins/ml2/ml2_conf.ini \
      upgrade head" neutron
    when: neutrontablecount.stdout|int < 10
    tags: neutron

  - name: ensure br_netfilter kernel module is available
    modprobe:
      name: br_netfilter
      state: present
    tags: neutron


#######################################################################
## Openstack-dashboard
  - name: configure Openstack-dashboard
    template:
      src=files/openstack-dashboard/local_settings.py.j2
      dest=/etc/openstack-dashboard/local_settings.py
      backup=yes
    notify: 
      - restart-apache2



  handlers:
  - name: reload-haproxy
    service: name=haproxy state=reloaded
  - name: restart-mysql
    service: name=mysql state=restarted
  - name: restart-memcached
    service: name=memcached state=restarted
  - name: restart-redis
    service: name=redis state=restarted
  - name: restart-etcd
    service: name=etcd state=restarted
  - name: restart-apache2
    service: name=apache2 state=restarted

  - name: restart-glance-registry
    service: name=glance-registry state=restarted
  - name: restart-glance-api
    service: name=glance-api state=restarted

  - name: restart-nova-api
    service: name="nova-api" state=restarted
  - name: restart-nova-conductor
    service: name="nova-conductor" state=restarted
  - name: restart-nova-scheduler
    service: name="nova-scheduler" state=restarted
  - name: restart-nova-novncproxy
    service: name="nova-novncproxy" state=restarted
  # - name: restart-nova-spiceproxy
    # service: name="nova-spiceproxy" state=restarted

  - name: restart-neutron-server
    service: name="neutron-server" state=restarted
  - name: restart-neutron-linuxbridge-agent
    service: name="neutron-linuxbridge-agent" state=restarted
  - name: restart-neutron-dhcp-agent
    service: name="neutron-dhcp-agent" state=restarted
  - name: restart-neutron-metadata-agent
    service: name="neutron-metadata-agent" state=restarted
  - name: restart-neutron-l3-agent
    service: name="neutron-l3-agent" state=restarted

#######################################################################
## Gnocchi
- hosts: gnocchi-api
  tasks:
## pakages
  - name: install gnocchi packages
    apt:  pkg={{packages}} state=present update_cache=yes
    tags: gnocchi

  - name: chown /var/lib/gnocchi directory
    file:
      path: /var/lib/gnocchi/tmp
      state: directory
      owner: gnocchi
      group: gnocchi
      mode: 0775

  
  - name: configure gnocchi
    template:
      src=files/{{item}}.j2
      dest=/etc/{{item}}
      backup=yes
    with_items:
      - gnocchi/gnocchi.conf
    notify: 
      - init-gnocchi
      # - restart-gnocchi-api
      - restart-gnocchi-metricd
      - restart-apache2
    tags: gnocchi

  handlers:
  - name: restart-apache2
    service: name=apache2 state=restarted
  - name: init-gnocchi
    shell: gnocchi-upgrade
  # - name: restart-gnocchi-api
  #   service: name="gnocchi-api" state=restarted
  - name: restart-gnocchi-metricd
    service: name="gnocchi-metricd" state=restarted


#######################################################################
## Ceilometer
- hosts: control
  tasks:
  - name: configure ceilometer
    template:
      src=files/{{item}}.j2
      dest=/etc/{{item}}
      backup=yes
    with_items:
      - ceilometer/ceilometer.conf
      - ceilometer/pipeline.yaml
    notify: 
      - init-ceilometer
      - restart-ceilometer-agent-central
      - restart-ceilometer-agent-notification
    tags: ceilometer

  handlers:
  - name: init-ceilometer
    shell: ceilometer-upgrade
  - name: restart-ceilometer-agent-central
    service: name="ceilometer-agent-central" state=restarted
  - name: restart-ceilometer-agent-notification
    service: name="ceilometer-agent-notification" state=restarted

#######################################################################
## Compute nodes
#######################################################################

- hosts: compute
  tasks:
## pakages
  - name: install copute-node packages
    apt:  pkg={{packages}} state=present update_cache=yes
    tags: compute-node

## Configure ceilometer
  - name: configure ceilometer
    template:
      src=files/{{item}}.j2
      dest=/etc/{{item}}
      backup=yes
    with_items:
      - ceilometer/ceilometer.conf
    notify: 
      - restart-ceilometer-agent-compute
      # - restart-ceilometer-agent-ipmi
      - restart-nova-compute
    tags:
      - ceilometer
      - compute-node

## Configure nova & neutron
  - name: configure nova & neutron
    template:
      src=files/{{item}}.j2
      dest=/etc/{{item}}
      backup=yes
    with_items:
      - nova/nova.conf
      - nova/nova-compute.conf
      - neutron/neutron.conf
      - neutron/plugins/ml2/linuxbridge_agent.ini
    notify: 
      - restart-neutron-linuxbridge-agent
      - restart-nova-compute
    tags:
      - compute-node
      - nova
      - neutron

  - name: ensure br_netfilter kernel module is available
    modprobe:
      name: br_netfilter
      state: present
    tags: compute-node

  handlers:
  - name: restart-ceilometer-agent-compute
    service: name=ceilometer-agent-compute state=restarted
  # - name: restart-ceilometer-agent-ipmi
  #   service: name=ceilometer-agent-ipmi state=restarted
  - name: restart-neutron-linuxbridge-agent
    service: name=neutron-linuxbridge-agent state=restarted
  - name: restart-nova-compute
    service: name=nova-compute state=restarted


#######################################################################
## Finish up control nodes (
## Use hosts: control (internal ip) because thats where the metadata is
#######################################################################

- hosts: control
  tasks:
  - name: make sure all services are started
    service: 
      name={{item}} 
      state=started
    with_items:
      - apache2 
      - chrony 
      - etcd 
      - glance-api 
      - glance-registry 
      - memcached 
      - mysql 
      - "neutron-dhcp-agent" 
      - "neutron-l3-agent" 
      # - "neutron-linuxbridge-agent" 
      - "neutron-metadata-agent" 
      - "neutron-server" 
      - "nova-api" 
      - "nova-conductor" 
      - "nova-novncproxy" 
      - "nova-scheduler" 
    tags: finalize

  - name: add compute nodes to cell database
    shell: |
      . ~/.adminrc && su -s /bin/sh -c "nova-manage cell_v2 \
      discover_hosts --verbose" nova
    tags:
      - compute
      - firstrun

  ### werkt niet om een of andere reden, endpoint not found?
  # - name: Create tiny flavor
  #   os_nova_flavor:
  #     cloud: "{{ooo}}"
  #     state: present
  #     name: tiny3
  #     ram: 512
  #     vcpus: 1
  #     ephemeral: 10
  #     disk: 10
  #     endpoint_type: public
  #   tags: finalize

  - name: Create a net-public
    os_network:
      cloud: "{{ooo}}"
#      state: present
      name: net-public
      project: admin
      provider_network_type: flat
      provider_physical_network: provider
#      provider_segmentation_id:
      shared: true
      external: true
    tags: finalize

  - name: Create subnet-public
    os_subnet:
      cloud: "{{ooo}}"
      name: subnet-public
      network_name: "net-public"
#      host_routes: 
      gateway_ip: "{{ pub_gw }}"
      # allocation_pool_start:
      # allocation_pool_end:
      cidr: "{{ pub_cidr }}"

      # dns_nameservers:
    tags: finalize

  - name: upload cirros image
    os_image:
      cloud: "{{ooo}}"
      name: cirros
      container_format: bare
      disk_format: qcow2
      state: present
      filename: http://download.cirros-cloud.net/0.4.0/cirros-0.4.0-x86_64-disk.img
      # filename: ~/cirros-0.4.0-x86_64-disk.img
      kernel: cirros-vmlinuz
      ramdisk: cirros-initrd
      properties:
        cpu_arch: x86_64
        distro: cirros
    tags: finalize


  # - name: download cirros image
  #   get_url:
  #     url: http://download.cirros-cloud.net/0.4.0/cirros-0.4.0-x86_64-disk.img
  #     dest: ~/cirros-0.4.0-x86_64-disk.img
  #   tags: finalize

  # - name: upload cirros image
  #   shell: |
  #     . ~/.adminrc && openstack image create "cirros" \
  #       --file ~/cirros-0.4.0-x86_64-disk.img --disk-format qcow2 \
  #       --container-format bare --public
  #   tags: finalize

  - name: get openstack services
    shell: |
      . ~/.adminrc 
      openstack image list
      openstack domain list
      openstack project list
      openstack user list
      openstack compute service list
      openstack image list
      openstack endpoint list --sort-column "Service Name" --sort-column "Interface"
    register: show_result
    tags: 
      - show_result
      - finalize

  - debug: var=show_result.stdout_lines
    tags: 
      - show_result
      - finalize

  - debug: 
      msg:
      - "domain          = default"
      - "admin user pass = {{ ADMIN_PASS }}"
      - "myuser user pass= {{ MYUSER_PASS }}"
      - "demo user pass  = {{ DEMO_PASS }}"
      - "horizon url     = http://{{control_node_ip_remote}}/horizon"
    tags: 
      - show_result
      - finalize
